{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upEJK8UUDnyn"
   },
   "source": [
    "> ### EEE4423: Deep Learning Lab\n",
    "\n",
    "# LAB \\#12: Sequence to Sequence Network with Attention Module\n",
    "## Machine Translation with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAv1aaG8Dnys"
   },
   "source": [
    "<h4><div style=\"text-align: right\"> Due date: May 27, 2022. </div> <br>\n",
    "<div style=\"text-align: right\"> Please upload your file @ LearnUs by 9 AM in the form of [ID_Name_Lab12.ipynb]. </div></h4>\n",
    "\n",
    "### *Instructions:*\n",
    "- Write a program implementing a particular algorithm to solve a given problem.   \n",
    "- <span style=\"color:red\">**Report and discuss your results. Analyze the algorithm, theoretically and empirically.**</span> \n",
    "- Each team must write their own answers and codes (<span style=\"color:red\">**if not you will get a F grade**</span>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SULIl9bRDnys"
   },
   "source": [
    "<h2><span style=\"color:blue\">[2017142136] [이관희]</span> </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2JP0LD9nDnys",
    "outputId": "0c1a8aae-4843-435b-dbaa-9624d918ff03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This code is written at 2022-05-26 02:09:06.706751\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(\"This code is written at \" + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBvC_gonDnyu"
   },
   "source": [
    "In this project we will be teaching a neural network to translate from\n",
    "French to English.\n",
    "*************************************************************\n",
    "::\n",
    "\n",
    "    [(>): input, (=): target, (<): output]\n",
    "\n",
    "    > il est en train de peindre un tableau .\n",
    "    = he is painting a picture .\n",
    "    < he is painting a picture .\n",
    "\n",
    "    > pourquoi ne pas essayer ce vin delicieux ?\n",
    "    = why not try that delicious wine ?\n",
    "    < why not try that delicious wine ?\n",
    "\n",
    "    > elle n est pas poete mais romanciere .\n",
    "    = she is not a poet but a novelist .\n",
    "    < she not not a poet but a novelist .\n",
    "\n",
    "    > vous etes trop maigre .\n",
    "    = you re too skinny .\n",
    "    < you re all alone .\n",
    "\n",
    "...\n",
    "*************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nLSIcUoZDnyu"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6v0s8XW7Dnyv"
   },
   "source": [
    "### 1. Prepare data\n",
    "\n",
    "The data for this project is a set of many thousands of English to French translation pairs. Download the data from <https://download.pytorch.org/tutorial/data.zip>. The file is a tab separated list of translation pairs:\n",
    "\n",
    "\n",
    "    I am cold.    J'ai froid.\n",
    "    \n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1K3W2RxeTKih5IiT5PcIyWNZSwMqtYSGZ\"  onerror=\"this.style.display='none'\" style=\"width: 600px;\"/><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Xn2soUCGDnyv",
    "outputId": "a76ed812-6017-4ceb-a52c-2fd70a6b1377"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 135842 sentence pairs\n",
      "Trimmed to 10599 sentence pairs\n",
      "Counted words: fra = 4345 eng = 2803\n",
      "['vous etes fort occupee .', 'you re very busy .']\n"
     ]
    }
   ],
   "source": [
    "SOS_token = 0 # 문장의 시작을 의미함\n",
    "EOS_token = 1 # 문장의 끝을 의미함\n",
    "\n",
    "# 단어를 index로 변환하는 문제이다.\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words # 그 단어 넣기\n",
    "            self.word2count[word] = 1 # 몇개가 온지 숫자 세기\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "# 일부 문장 만 선택\n",
    "            \n",
    "MAX_LENGTH = 10\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \")\n",
    "            \n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join( c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "# 질문 : 무슨 뜻인지 이해가 되지 않습니다. -> 그냥\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip()) # 빈 문자 제거\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s) # text_mod = re.sub('apble|abple',\"apple\",text) 변경\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "# 질문 : 이게 왜 normalize 이지? -> \n",
    "\n",
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    # Read the file and split into lines\n",
    "    lines = open('../dataset/lab12/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "# 데이터 준비\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        # 질문 : pair[0]를 의미하는 것\n",
    "        output_lang.addSentence(pair[1])\n",
    "        # 질문 : pair[1]을 의미하는 것\n",
    "    print(\"Counted words:\", input_lang.name, '=', input_lang.n_words, output_lang.name, '=', output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "A8yVC59WDnyw"
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4qXkB3DDnyw"
   },
   "source": [
    "### 2. Build the Seq2Seq model [5 points]\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1kKXrIIxi0t-Nm5HfzOukqjzEp7yEXEpV\"  onerror=\"this.style.display='none'\" /><br><br>\n",
    "\n",
    "[sequence to sequence network](https://arxiv.org/abs/1409.3215) is a model in which two\n",
    "recurrent neural networks work together to transform one sequence to\n",
    "another. An encoder network condenses an input sequence into a single vector,\n",
    "and a decoder network unfolds that vector into a new sequence.\n",
    "\n",
    "Unlike sequence prediction with a single RNN, where every input\n",
    "corresponds to an output, the seq2seq model frees us from sequence\n",
    "length and order, which makes it ideal for translation between two\n",
    "languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDTwMr5TDnyy"
   },
   "source": [
    "#### Encoder\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for every word from the input sentence. For every input word the encoder outputs a vector and a hidden state, and uses the hidden state for the next input word.  \n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1PyKBEVl5jwQfB0I0P2kG8nTGQVZQdZEM\"  onerror=\"this.style.display='none'\" /><br><br>\n",
    "\n",
    "#### GRU\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1467jVFRYbw1DYvVKeSyzGWLRmtlqpy8z\"  onerror=\"this.style.display='none'\" style=\"width: 700px;\"/><br><br>\n",
    "The GRU operates using a reset gate (r) and an update gate (z). The candidate state is created by using the previous hidden state and the current input. It is the reset gate that determines how the previous hidden state affects the candidate state. The newly created candidate state and the previous hidden state create a new hidden state, in which the update gate plays a role in balancing the two.\n",
    "\n",
    "#### LSTM vs GRU\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1lzGTsIYvPWKNF-XaTevMaaZvjfgp9G35\"  onerror=\"this.style.display='none'\" style=\"width: 600px;\"/><br><br>\n",
    "\n",
    "| <center>LSTM</center> | <center>GRU</center>  |\n",
    "|:--------|--------|\n",
    "| LSTM has 3 gates (forget, input, output) | GRU has 2 gates (reset, update) |\n",
    "| There is an internal memory (cell state) | There is no cell state and only hidden state exists |\n",
    "| When making output, another non-linearity is applied | There is no additional non-linearity when making output  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module) :\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(GRU, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # reset gate\n",
    "        self.reset_gate_x = nn.Linear(input_dim, hidden_dim)\n",
    "        self.reset_gate_h = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        #update_gate\n",
    "        self.update_gate_x = nn.Linear(input_dim, hidden_dim)\n",
    "        self.update_gate_h = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # h gate\n",
    "        self.h_gate_x = nn.Linear(input_dim, hidden_dim)\n",
    "        self.h_gate_h = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        ##output_gate\n",
    "        self.output_gate_x = nn.Linear(input_dim,hidden_dim)\n",
    "        self.output_gate_h = nn.Linear(hidden_dim,hidden_dim)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.decoder = nn.Linear(hidden_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, x , hn) :\n",
    "        ## update gate -> z_t\n",
    "        z_t=self.sigmoid(self.update_gate_x(x)+self.update_gate_h(hn)) # [1,1,100]\n",
    "        \n",
    "        ## reset gate -> r_t\n",
    "        r_t=self.sigmoid(self.reset_gate_x(x)+self.reset_gate_h(hn))      #[1,1,100]  \n",
    "        tillde_h = self.tanh(self.h_gate_x(x)+self.h_gate_h(hn*r_t)) # [1,1,100]\n",
    "        \n",
    "        ## h_t\n",
    "        h_t = (1-z_t)*tillde_h + z_t*hn\n",
    "        \n",
    "        output = self.decoder(h_t.view(1,-1)) # [1,100]\n",
    "        output = output.unsqueeze(0)\n",
    "        return output, h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PpL3bajyDnyy"
   },
   "outputs": [],
   "source": [
    "# 2 points\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(input_dim, hidden_dim)\n",
    "        # gru\n",
    "        # The size of input is (batch_size, seq_dim, hidden_dim)\n",
    "        ## gru 부분 다시 설계할 필요가 있다. -> module로 구현하자.\n",
    "#         self.gru = nn.GRU(hidden_dim, hidden_dim)\n",
    "        self.gru = GRU(hidden_dim, hidden_dim)\n",
    "    def forward(self, input, hn):\n",
    "        ##\n",
    "        # print(input.size()) # torch.Size([1])\n",
    "        k = self.embedding(input)\n",
    "        # print(k.size()) # torch.Size([1, 256])\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        # print(embedded.size()) # torch.Size([1, 1, 256])\n",
    "        output, hn = self.gru(embedded, hn)\n",
    "        # print(output.size()) # torch.Size([1, 1, 256])\n",
    "#         print(hn.size()) # torch.Size([1, 1, 256])\n",
    "        return output, hn\n",
    "\n",
    "    def initHidden(self):\n",
    "        # The size of h0 should be (layer_dim, batch_size, hidden_dim)\n",
    "        #############\n",
    "        h0 = torch.zeros(1, 1, self.hidden_dim, device=device)\n",
    "        return h0\n",
    "    \n",
    "hidden_dim = 256\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4345"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lang.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pairs = [tensorsFromPair(random.choice(pairs)) for iter in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = training_pairs[4][0]\n",
    "target = training_pairs[4][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.cuda.LongTensor'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = encoder.initHidden()\n",
    "input_length = input.size(0)\n",
    "encoder_outputs = torch.zeros(10, encoder.hidden_dim, device=device)\n",
    "\n",
    "for ei in range(input_length):\n",
    "    encoder_output, encoder_hidden = encoder(input[ei], h)\n",
    "    encoder_outputs[ei] = encoder_output[0,0]\n",
    "#     break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 256])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output[0,0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderRNN(\n",
       "  (embedding): Embedding(4345, 256)\n",
       "  (gru): GRU(\n",
       "    (reset_gate_x): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (reset_gate_h): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (update_gate_x): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (update_gate_h): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (h_gate_x): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (h_gate_h): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (output_gate_x): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (output_gate_h): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (sigmoid): Sigmoid()\n",
       "    (tanh): Tanh()\n",
       "    (decoder): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGjPPHD7Dnyy"
   },
   "source": [
    "#### Decoder\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1Rm_LlpEolCvPuzPWEFOZ-zdTfsgMbtu-\"  onerror=\"this.style.display='none'\" /><br><br>\n",
    "\n",
    "If only the context vector is passed betweeen the encoder and decoder, that single vector carries the burden of encoding the entire sentence. Attention allows the decoder network to \"focus\" on a specific part of\n",
    "the encoder's outputs for every step and thus help the decoder choose the right output words. \n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=18hsS8PAA7I3QaN9oOebfnMGAMhR-6EID\"  onerror=\"this.style.display='none'\" style=\"width: 170px;\"/><br><br>\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1F1Y92uLvGaI6s-ygyNKNox4ZGiZmTZ3g\"  onerror=\"this.style.display='none'\" style=\"width: 170px;\"/><br><br>\n",
    "\n",
    "The attention weights are calculated using an another feed-forward layer which inputs the decoder's input and hidden state. And the calculated attention weight is multiplied to the corresponding hidden state of the encoder, respectively. Note that to actually create and train this layer we have to choose a maximum sentence length. Sentences of the maximum length will use all the attention weights, while shorter sentences will only use the first few.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1JEE23gtJf4XciJUXLt2R9lZtpRn8mYCN\"  onerror=\"this.style.display='none'\" /><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "NfWr22rFDnyz"
   },
   "outputs": [],
   "source": [
    "# 3 points\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_dim, self.hidden_dim)\n",
    "#         self.embedding = nn.Embedding(1, self.hidden_dim) -> toy example\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        \n",
    "        # attention\n",
    "        # Note that the column of the attention weights is MAX_LENGTH\n",
    "        # Note that concatenation is used when \"attn\" and \"attn_combine\" are created\n",
    "        #############\n",
    "        # attention은 결국 적절한 hidden_dim에 어떤 값을 곱해주는 것이다.\n",
    "        self.attention = nn.Linear(self.hidden_dim * 2, MAX_LENGTH) # MAX_LENGTH = 10\n",
    "        self.attention_combine = nn.Linear(self.hidden_dim * 2, self.hidden_dim)\n",
    "        \n",
    "        # gru\n",
    "        # The size of input is (batch_size, seq_dim, hidden_dim)\n",
    "        #############\n",
    "#         self.gru = nn.GRU(self.hidden_dim, self.hidden_dim)\n",
    "        self.gru = GRU(self.hidden_dim, self.hidden_dim)\n",
    "\n",
    "        self.out = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "    def forward(self, input, hn, encoder_outputs):\n",
    "        input = self.embedding(input).view(1, 1, -1) # 이렇게 해야 1,1,256 이 됨.\n",
    "        input = self.dropout(input)\n",
    "        \n",
    "        # attention\n",
    "        # All specifications of the operations are described in the above figure (e.g. use ReLU)\n",
    "        # bmm is a operation which performs a batch matrix-matrix product\n",
    "        #############\n",
    "        embedded = input\n",
    "#         print(embedded.size()) # torch.Size([1, 1, 256])\n",
    "        attn_weights = F.softmax(\n",
    "            self.attention(torch.cat((embedded[0], hn[0]), 1)), dim=1) # 1,512 -> 1,10 \n",
    "        #\n",
    "#         print(attention_weights.size()) # torch.Size([1, 10]) toy example\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "        # print(attn_applied.size()) -> torch.Size([1, 1, 256])\n",
    "        \n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        attn_combine = self.attention_combine(output).unsqueeze(0) # [1,1,256]\n",
    "        \n",
    "        output = F.relu(attn_combine)\n",
    "        \n",
    "        # gru\n",
    "        #############\n",
    "        output, hidden = self.gru(output, hn)\n",
    "        #print(output.size()) # torch.Size([1, 256]) -> 내가 만든 GRU\n",
    "        # print(output.size()) # torch.Size([1, 1, 256]) -> nn.GRU\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        # print(output.size()) # torch.Size([1, 2803])\n",
    "        return output, hn\n",
    "\n",
    "    def initHidden(self):\n",
    "        # The size of h0 should be (layer_dim, batch_size, hidden_dim)\n",
    "        ############# 없어도 됨\n",
    "        h0 = torch.zeros(1, 1, self.hidden_dim, device=device)\n",
    "        return h0\n",
    "    \n",
    "decoder = AttnDecoderRNN(hidden_dim, output_lang.n_words, dropout_p=0.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_hidden = encoder_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_TOKEN = 0\n",
    "decoder_input = torch.tensor([[SOS_TOKEN]], device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2803"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lang.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(1, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decoder_output, decoder_hidden  = decoder(decoder_input, decoder_hidden, encoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kg2utJ-1Dnyz"
   },
   "source": [
    "### 3. Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "KE-npWCODnyz"
   },
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "\n",
    "learning_rate=0.01\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMr9RcDBDnyz"
   },
   "source": [
    "### 4. Write the evaluation code [2 points]\n",
    "\n",
    "- Using the trained model, display the translated output given input sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Lang at 0x7f5fbea74f40>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "UO3UzfFCDnyz"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    with torch.no_grad():\n",
    "        # tensor을 문장으로 바꾸자.\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        \n",
    "        #### 값 초기화\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        \n",
    "        encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_dim, device=device)\n",
    "        \n",
    "        \n",
    "        #### encoder의 output을 저장\n",
    "        for i in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[i],encoder_hidden)\n",
    "            encoder_outputs[i] += encoder_output[0, 0]\n",
    "        # 위에처럼 구성하면 encoder의 모든 output을 저장할 수 있다. \n",
    "        \n",
    "        # decoder의 초기값 저장\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        \n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        decoded_words = []\n",
    "        \n",
    "        #############\n",
    "        for di in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # topk는 지정된 차원을 따라 지정된 텐서의 가장 큰 요소를 반환합니다.\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            # top value = topv , top index = topi를 의미한다.\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>') # <EOS> 츨력하기 싫으면 이 부분만 주석하면 된다.\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "                # decoded_words에 적절한(최대의 확률을 가지는) 단어가 대입 됨. \n",
    "\n",
    "            decoder_input = topi.squeeze().detach() # topidex의 값이 단어로써 들어가게된다.\n",
    "\n",
    "        return decoded_words\n",
    "    \n",
    "def evaluateRandomly():\n",
    "    pair = random.choice(pairs)\n",
    "    print('>', pair[0])\n",
    "    print('=', pair[1])\n",
    "    output_words = evaluate(pair[0])\n",
    "    output_sentence = ' '.join(output_words)\n",
    "    print('<', output_sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> vous etes celle qui m a formee .\n",
      "= you re the one who trained me .\n",
      "< somewhere rent stylish stylish real milk milk milk milk somewhere\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7wNENhXDnyz"
   },
   "source": [
    "### 5 . Write the code to train the model [3 points]\n",
    "\n",
    "- During training, use the `Teacher forcing` concept in addition to a naive approach.\n",
    "    - In other words, instead of using the decoder's guess as the next input, the real target outputs are also used sometimes. This shows faster convergence.\n",
    "- Plot the training loss curve.\n",
    "- Show the result using $evaluateRandomly()$ function. Below is an example.\n",
    "*************************************************************\n",
    "    > il est en train de peindre un tableau . (input)\n",
    "    = he is painting a picture . (target)\n",
    "    < he is painting a picture . (output)\n",
    "*************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "9x83bK3GDnyz",
    "outputId": "34533830-f22b-4080-bc51-582d4993b8e2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************* iter1000 *************************\n",
      "loss 36.7561\n",
      "> je suis desolee de t avoir blesse .\n",
      "= i m sorry i hurt you .\n",
      "< i m not . <EOS>\n",
      "\n",
      "************************* iter2000 *************************\n",
      "loss 23.8050\n",
      "> tu es excuse .\n",
      "= you re forgiven .\n",
      "< you re very . <EOS>\n",
      "\n",
      "************************* iter3000 *************************\n",
      "loss 15.5773\n",
      "> il est fort en colere .\n",
      "= he is very angry .\n",
      "< he s a . <EOS>\n",
      "\n",
      "************************* iter4000 *************************\n",
      "loss 10.9581\n",
      "> je suis content de vous voir ici .\n",
      "= i m glad to see you here .\n",
      "< i m glad you you . <EOS>\n",
      "\n",
      "************************* iter5000 *************************\n",
      "loss 11.1328\n",
      "> il est normalement chez lui le soir .\n",
      "= he is generally at home in the evening .\n",
      "< he is a to . <EOS>\n",
      "\n",
      "************************* iter6000 *************************\n",
      "loss 6.9102\n",
      "> il est tres comprehensif .\n",
      "= he s very understanding .\n",
      "< he is very good . <EOS>\n",
      "\n",
      "************************* iter7000 *************************\n",
      "loss 21.9904\n",
      "> je me rejouis que nous l ayons fait .\n",
      "= i m glad we did that .\n",
      "< i m glad you . <EOS>\n",
      "\n",
      "************************* iter8000 *************************\n",
      "loss 10.9014\n",
      "> je suis fatigue de lire .\n",
      "= i m sick and tired of reading .\n",
      "< i m afraid of . <EOS>\n",
      "\n",
      "************************* iter9000 *************************\n",
      "loss 7.4600\n",
      "> tu es productif .\n",
      "= you re productive .\n",
      "< you re being . <EOS>\n",
      "\n",
      "************************* iter10000 *************************\n",
      "loss 19.6499\n",
      "> il est plus fort que moi .\n",
      "= he s stronger than me .\n",
      "< he s smarter than me . <EOS>\n",
      "\n",
      "************************* iter11000 *************************\n",
      "loss 19.3000\n",
      "> je ne vais pas partir avec toi .\n",
      "= i m not leaving with you .\n",
      "< i m not going to be you m not going\n",
      "\n",
      "************************* iter12000 *************************\n",
      "loss 16.8982\n",
      "> elles sont mes amies .\n",
      "= they are my friends .\n",
      "< they are my friends . <EOS>\n",
      "\n",
      "************************* iter13000 *************************\n",
      "loss 5.1911\n",
      "> tu es fort contrariee .\n",
      "= you re very upset .\n",
      "< you re very upset . <EOS>\n",
      "\n",
      "************************* iter14000 *************************\n",
      "loss 15.3287\n",
      "> je reste .\n",
      "= i m staying .\n",
      "< i m looking to do . <EOS>\n",
      "\n",
      "************************* iter15000 *************************\n",
      "loss 8.2590\n",
      "> je suis amoureux de vous .\n",
      "= i m in love with you .\n",
      "< i m in . <EOS>\n",
      "\n",
      "************************* iter16000 *************************\n",
      "loss 14.9048\n",
      "> nous ne sommes pas ouvertes .\n",
      "= we re not open .\n",
      "< we re not a . <EOS>\n",
      "\n",
      "************************* iter17000 *************************\n",
      "loss 3.6655\n",
      "> je ne suis pas nerveux .\n",
      "= i m not nervous .\n",
      "< i m not normal . <EOS>\n",
      "\n",
      "************************* iter18000 *************************\n",
      "loss 27.0742\n",
      "> je me trouve dans une situation desesperee .\n",
      "= i m in a desperate situation .\n",
      "< i m taking a <EOS>\n",
      "\n",
      "************************* iter19000 *************************\n",
      "loss 5.5422\n",
      "> je suis gene .\n",
      "= i m embarrassed .\n",
      "< i m . <EOS>\n",
      "\n",
      "************************* iter20000 *************************\n",
      "loss 6.0533\n",
      "> vous etes astucieux .\n",
      "= you re crafty .\n",
      "< you re crafty . <EOS>\n",
      "\n",
      "************************* iter21000 *************************\n",
      "loss 18.5417\n",
      "> j eprouve du ressentiment .\n",
      "= i m resentful .\n",
      "< i am from . <EOS>\n",
      "\n",
      "************************* iter22000 *************************\n",
      "loss 11.6188\n",
      "> tu regardes tout le temps la tele .\n",
      "= you are always watching tv .\n",
      "< you re always watching tv <EOS>\n",
      "\n",
      "************************* iter23000 *************************\n",
      "loss 1.7381\n",
      "> nous sommes tot a la maison .\n",
      "= we re home early .\n",
      "< we re home home home home home home home home\n",
      "\n",
      "************************* iter24000 *************************\n",
      "loss 18.3906\n",
      "> j adore prendre des photos .\n",
      "= i m fond of taking pictures .\n",
      "< i m playing <EOS>\n",
      "\n",
      "************************* iter25000 *************************\n",
      "loss 10.7270\n",
      "> vous n etes pas ma mere .\n",
      "= you aren t my mother .\n",
      "< you aren . <EOS>\n",
      "\n",
      "************************* iter26000 *************************\n",
      "loss 6.2127\n",
      "> il est bon en anglais .\n",
      "= he is a good speaker of english .\n",
      "< he s good at english . <EOS>\n",
      "\n",
      "************************* iter27000 *************************\n",
      "loss 0.7060\n",
      "> je ne suis pas perfectionniste .\n",
      "= i m not a perfectionist .\n",
      "< i m not a . <EOS>\n",
      "\n",
      "************************* iter28000 *************************\n",
      "loss 5.8348\n",
      "> nous allons faire tout ce que nous pouvons .\n",
      "= we re going to do everything we can .\n",
      "< we re going to <EOS>\n",
      "\n",
      "************************* iter29000 *************************\n",
      "loss 0.8551\n",
      "> c est un voleur .\n",
      "= he is a thief .\n",
      "< he is a thief . <EOS>\n",
      "\n",
      "************************* iter30000 *************************\n",
      "loss 1.2711\n",
      "> il est genereux avec son argent .\n",
      "= he is generous with his money .\n",
      "< he is generous with his . <EOS>\n",
      "\n",
      "************************* iter31000 *************************\n",
      "loss 5.9472\n",
      "> tu frissonnes .\n",
      "= you re shivering .\n",
      "< you re turning . <EOS>\n",
      "\n",
      "************************* iter32000 *************************\n",
      "loss 22.3319\n",
      "> c est un artiste celebre .\n",
      "= he is a famous artist .\n",
      "< he s a famous . <EOS>\n",
      "\n",
      "************************* iter33000 *************************\n",
      "loss 0.7218\n",
      "> tu passes a cote .\n",
      "= you re missing the point .\n",
      "< you re missing the point . <EOS>\n",
      "\n",
      "************************* iter34000 *************************\n",
      "loss 0.4941\n",
      "> je suis en train de regarder la television .\n",
      "= i m watching tv .\n",
      "< i m watching tv tv tv tv tv tv tv\n",
      "\n",
      "************************* iter35000 *************************\n",
      "loss 12.2004\n",
      "> tu es vraiment embetante .\n",
      "= you re really annoying .\n",
      "< you re really . <EOS>\n",
      "\n",
      "************************* iter36000 *************************\n",
      "loss 5.3950\n",
      "> je suis de singapour .\n",
      "= i m from singapore .\n",
      "< i m from . <EOS>\n",
      "\n",
      "************************* iter37000 *************************\n",
      "loss 0.3421\n",
      "> je travaille pour une entreprise de commerce .\n",
      "= i m working for a trading firm .\n",
      "< i m an father a trip . <EOS>\n",
      "\n",
      "************************* iter38000 *************************\n",
      "loss 8.5742\n",
      "> je suis voyant .\n",
      "= i m psychic .\n",
      "< i m psychic . <EOS>\n",
      "\n",
      "************************* iter39000 *************************\n",
      "loss 4.5971\n",
      "> c est l un des meilleurs .\n",
      "= he s one of the best .\n",
      "< he is one of the best . <EOS>\n",
      "\n",
      "************************* iter40000 *************************\n",
      "loss 12.5417\n",
      "> vous etes belle .\n",
      "= you are beautiful .\n",
      "< you re beautiful . <EOS>\n",
      "\n",
      "************************* iter41000 *************************\n",
      "loss 0.2535\n",
      "> tu es a court de sucre .\n",
      "= you re out of sugar .\n",
      "< you re out of <EOS>\n",
      "\n",
      "************************* iter42000 *************************\n",
      "loss 0.0246\n",
      "> c est vous les proprietaires .\n",
      "= you re the owners .\n",
      "< you re the owners the owners the owners the owners\n",
      "\n",
      "************************* iter43000 *************************\n",
      "loss 0.5421\n",
      "> elle est belle comme blanche neige .\n",
      "= she is as beautiful as snow white .\n",
      "< she s as beautiful as beautiful beautiful as beautiful as\n",
      "\n",
      "************************* iter44000 *************************\n",
      "loss 2.4155\n",
      "> je ne suis pas millionnaire .\n",
      "= i m not a millionaire .\n",
      "< i m not . <EOS>\n",
      "\n",
      "************************* iter45000 *************************\n",
      "loss 8.9918\n",
      "> je suis plus astucieuse que vous .\n",
      "= i m smarter than you .\n",
      "< i m smarter than you . <EOS>\n",
      "\n",
      "************************* iter46000 *************************\n",
      "loss 9.3640\n",
      "> je dors peu .\n",
      "= i m a light sleeper .\n",
      "< i am a . <EOS>\n",
      "\n",
      "************************* iter47000 *************************\n",
      "loss 32.4715\n",
      "> tu es gare en double file .\n",
      "= you re double parked .\n",
      "< you re double parked . <EOS>\n",
      "\n",
      "************************* iter48000 *************************\n",
      "loss 7.4588\n",
      "> vous n etes pas toubib .\n",
      "= you re not a doctor .\n",
      "< you are not a doctor . <EOS>\n",
      "\n",
      "************************* iter49000 *************************\n",
      "loss 6.3953\n",
      "> nous sommes des prisonniers .\n",
      "= we re prisoners .\n",
      "< we re prisoners . <EOS>\n",
      "\n",
      "************************* iter50000 *************************\n",
      "loss 4.0610\n",
      "> vous etes completement ignorantes .\n",
      "= you re totally ignorant .\n",
      "< you re totally ignorant . <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_iters = 50000\n",
    "# n_iters = 1000\n",
    "print_every = 1000\n",
    "plot_every =100\n",
    "\n",
    "plot_losses = []\n",
    "print_loss_total = 0  # Reset every print_every\n",
    "plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]\n",
    "\n",
    "for iter in range(1, n_iters+1):\n",
    "    # Load data\n",
    "    training_pair = training_pairs[iter-1]\n",
    "    input_tensor = training_pair[0]\n",
    "    target_tensor = training_pair[1]\n",
    "    \n",
    "    # Clear gradients w.r.t. parameters\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_dim, device=device) # 10 = MAX_LENGTH\n",
    "    loss = 0\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    # Forward pass\n",
    "    # encoder\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0,0]\n",
    "    \n",
    "    # decoder    \n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "            \n",
    "    is_teacher_forcing = True if random.random() < 0.5 else False # 밑에 두가지 경우를 확률적으로 1/2로 선택한다.\n",
    "    \n",
    "    # 여기서는 학습 데이터의 다음 입력을 받기 때문에 decoder이 잘못 예측해도 encoder,decoder 이 잘 학습된다.\n",
    "    if is_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            # 학습 데이터에서 다음 입력을 선택\n",
    "            decoder_input = target_tensor[di]\n",
    "    # 여기서는 decoder의 output으로 학습을 이어가기 때문에 decoder의 output이 잘 맞아야 encoder이 잘 학습된다.\n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            # decoder의 출력 값을 다음 입력을 선택 \n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach() \n",
    "            \n",
    "            if decoder_input.item() == EOS_token: # <EOS>가 나오면 반복문을 끝낸다.\n",
    "                break\n",
    "            \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Updating parameters\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    print_loss_total += loss.item() / target_length\n",
    "    plot_loss_total += loss.item() / target_length\n",
    "\n",
    "    if iter % print_every == 0:\n",
    "        print('*'*25, 'iter%d'%iter, '*'*25)\n",
    "        print('loss %.4f'%loss)\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        evaluateRandomly()\n",
    "\n",
    "    if iter % plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f04f2da63d0>]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA5q0lEQVR4nO3deXxU1dkH8N8zS2ayhywEwhaQTXYwIAjKIiqi1dZq1Vq1bpT31db6tlrt4tZarVq1Vmvr0lbUutatahUUUFABw75DgLCEkI3syUwyM+f94y5z7507k5kww2Qmz/fzyYeZe+/cOTfGZ84895znkBACjDHGEp8l3g1gjDEWHRzQGWMsSXBAZ4yxJMEBnTHGkgQHdMYYSxK2eL1xfn6+KC4ujtfbM8ZYQlq/fn2tEKLAbF/cAnpxcTFKS0vj9faMMZaQiOhgsH2ccmGMsSTBAZ0xxpIEB3TGGEsSHNAZYyxJcEBnjLEkwQGdMcaSBAd0xhhLEgkX0Hcfa8Yfl+5GXYs73k1hjLEeJeECell1C/68vAx1rR3xbgpjjPUoCRfQrXKLPV5emIMxxrQSMKBLTfbxSkuMMaaTgAFd+tfj44DOGGNaCRjQpSZ7OaAzxphO4gV0IgAc0BljzCjxArqFAzpjjJkJO6ATkZWINhLRByb7HET0OhGVEdFaIiqOais1OKAzxpi5SHrotwLYGWTfDQDqhRDDATwO4A8n2rBglJuiXh7lwhhjOmEFdCIaCOACAM8HOeRiAC/Kj98CcDaRnOyOMv9NUV8sTs8YYwkr3B76EwDuABAsig4AcBgAhBAeAI0A8owHEdEiIiolotKamprIWwvtTdFuvZwxxpJWlwGdiC4EUC2EWH+ibyaEeFYIUSKEKCkoMF3jtEucQ2eMMXPh9NBnAriIiMoBvAZgHhG9bDimAsAgACAiG4BsAHVRbKeKAzpjjJnrMqALIe4SQgwUQhQDuALAciHEDwyHvQ/gWvnxpfIxMYm4akDnm6KMMaZj6+4Lieh+AKVCiPcBvADgJSIqA3AcUuCPCX8PnZPojDGmFVFAF0KsBLBSfny3ZrsLwGXRbFgwNgvfFGWMMTMJN1PUwj10xhgzlXABnXvojDFmLuECuoW4h84YY2YSLqDbeNgiY4yZSriAruTQeYELxhjTS7iArvTQeQk6xhjTS7iAbuUeOmOMmUrYgO7jgM4YYzqJF9CJe+iMMWYm4QK6hXvojDFmKuECOiDdGOUeOmOM6SVkQLdYiKstMsaYQUIGdJuF4PVyQGeMMa2EDOhW4h46Y4wZJWZAtxLfFGWMMYPEDOjEN0UZY8woMQO6hXjqP2OMGSRsQPfwTVHGGNNJ2IDON0UZY0wvcQM659AZY0yny4BORE4iWkdEm4loOxHdZ3LMD4mohog2yT83xqa5Eg7ojDEWyBbGMW4A84QQLURkB7CaiP4rhFhjOO51IcQt0W9iICtxQGeMMaMuA7oQQgBokZ/a5Z+4RlPuoTPGWKCwcuhEZCWiTQCqASwTQqw1Oey7RLSFiN4iokFBzrOIiEqJqLSmpqbbjeaAzhhjgcIK6EIIrxBiEoCBAKYR0TjDIf8BUCyEmABgGYAXg5znWSFEiRCipKCgoNuNtvEoF8YYCxDRKBchRAOAFQAWGLbXCSHc8tPnAZwWldYFYeEeOmOMBQhnlEsBEeXIj1MBnANgl+GY/pqnFwHYGcU2BrBxQGeMsQDh9ND7A1hBRFsAfAMph/4BEd1PRBfJx/xEHtK4GcBPAPwwNs2VWAy1XN4oPYzqZlcs35Ixxnq8cEa5bAEw2WT73ZrHdwG4K7pNC+7Q8TZUNrqwZn8dirJTccdbW3DGKXn4103TT1YTGGOsx0nImaKTB+cAAHZWNqGuVUrdN7Z3xrFFjDEWfwkZ0B/73iQAwKq9tdh4qAEAkOEIZ44UY4wlr4SMgg6bBRYClu+qxvJd1QCATGdCXgpjjEVNQvbQiQjpKfoAnpbCAZ0x1rslZEAHALtN33S3xxunljDGWM+QsAHdOA69rYMDOmOsd0vYgG5cJJoDOmOst0vcgG6o5dLq9sSpJYwx1jMkbEA3FufadawZFQ3tcWoNY4zFX8IGdJ/P/3hkYQYA4Ow/roxPYxhjrAdI3IAu99A/+PEsjB+QAwBwdfpCvIIxxpJbwgZ0JeVSlJOK8rrWOLeGMcbiL2EDek6qHYA05X94gZRySbVb49kkxhiLq4SdXvnm4jOwem8NUmwW3HvRWOyracG+mpauX8gYY0kqYQP68L4ZGN5X7pmnWDF9WB42Hm5Ai9uDNLsVFgvFuYWMMXZyJWzKxSgr1QavT2DcPZ/g2VX7490cxhg76ZImoGfLOXUAWHfgeBxbwhhj8ZE0AT3L6Q/ohVmOOLaEMcbiI2kCulZTO5cBYIz1PkkT0OeO7otb5g7H8L4ZaHLxcnSMsd6ny4BORE4iWkdEm4loOxHdZ3KMg4heJ6IyIlpLRMUxaW0ITrsVPz9vFIpyUtHk4h46Y6z3CaeH7gYwTwgxEcAkAAuIaLrhmBsA1AshhgN4HMAfotrKCGQ5bWjmBaMZY71QlwFdSJQZO3b5RxgOuxjAi/LjtwCcTURxGQie6bSjtsWN332wA/WtHfFoAmOMxUVYOXQishLRJgDVAJYJIdYaDhkA4DAACCE8ABoB5JmcZxERlRJRaU1NzQk1PJisVBuaXB48v/oA/vTZ3pi8B2OM9URhBXQhhFcIMQnAQADTiGhcd95MCPGsEKJECFFSUFDQnVN0STt80cqzRRljvUhEo1yEEA0AVgBYYNhVAWAQABCRDUA2gLootC9i2glGL6w+gN9+sCMezWCMsZMunFEuBUSUIz9OBXAOgF2Gw94HcK38+FIAy4UQxjz7STG2KEv3/IXVB+LRDMYYO+nCKc7VH8CLRGSF9AHwhhDiAyK6H0CpEOJ9AC8AeImIygAcB3BFzFrchbFF2fF6a8YYi6suA7oQYguAySbb79Y8dgG4LLpN654UmwXzTy3EpzurpOfWpJk7xRhjISVltHv+2hL1sdOelJfIGGMBkj7aOXkVI8ZYL5H0AT01hQM6Y6x3SPqAfrCuDct2VMW7GYwxFnNJG9Dnje6rPr5pSSnKqnm9UcZYckvagP63q0/DRROL1OfzH/sci5aU4t73t8exVYwxFjsJu0h0V+xWCwoy9SsXLZVTL+eOLcQZp+THo1mMMRYzSdtDB4BglVw+3x2bwmCMMRZPSR3Q2zq9ptu3HW08yS1hjLHYS9qUCwC0d/gD+gvXlqCioR3bK5rw8fZjEEIgTiXbGWMsJpK6h97qlpaie/r7U3D2qYW4ZkYxRvXLRGN7J+rbOlHV5MLfVx9AV3XE1u6vwwdbjp6MJjPGWLcldQ/9immDsHRHFaYMyVG39ct2AgCONbpwz/vb8E15PcYWZcFpt2LioBzT81z+7BoAwIUTikz3M8ZYT5DUAX3e6EKUP3SBblthlhTQq5pdqG+T1h5VAvbmu88FSF9TnTHGEkVSB3QzhVnSUMaqRheshhz6xPuXAkDAhwBjjCWCXhfQ+2ZKPfQ7397KlRgZY0ml10W0FJv/kl2dvoheG6dFmBhjLCy9LqADwP0Xjw253+czD9wd3sg+ABhj7GTqlQH9mhnFyHAEzzY1tnfC6xPw+YSuVx5pj54xxk6mXhnQAaC/PHzRTGWjCxc/vRo3LSlFu2a2qSvIzFPGGOsJem9Az0kNuu/1bw5hW0UTPttVjcb2TnX7mv11J6NpjDHWLV0GdCIaREQriGgHEW0noltNjplDRI1EtEn+udvsXD3JsPz0oPu+2FurPt50qEF9fOtrm3C8tSOWzWKMsW4Lp4fuAfAzIcQYANMB3ExEY0yOWyWEmCT/3B/VVsbA9GG5QfcdqG2F1SKNUTf2ymua3TFtF2OMdVeXAV0IUSmE2CA/bgawE8CAWDcs1qYW6wP63FEFmDk8D/kZDvm5tOLRpzurdcdVN7tOTgMZYyxCEeXQiagYwGQAa012zyCizUT0XyIyHRdIRIuIqJSISmtq4luTPC/Dgc9vn4NNd5+D62cOxROXT8YrN05XR7VIwT0FFQ3tutdVNUk9dFenF795dxv32BljPUbYAZ2IMgD8G8BPhRBNht0bAAwRQkwE8GcA75qdQwjxrBCiRAhRUlBQ0M0mR8+QvHTkpKXg7m+NQXaaVL/l8qmDAACXlQxCWoo0tHHh+H7qa6qapB76G6WH8dKag3j2i30nudWMMWYurIBORHZIwfwVIcTbxv1CiCYhRIv8+CMAdiJKyDXefn7uKOy8fwEyHDa1/O6EgTnq/nc2VuDlNQexv6YVgNTT/2hrJd7fzOV1GWPxFc4oFwLwAoCdQojHghzTTz4ORDRNPm9CjvGzWAipKVYAQGuHFNDHFmWp+8uqW/Drd7fhYJ0U0O1WC/73lQ34yasbA87l9QnM++NKfLS18iS0nDHW24XTQ58J4GoA8zTDEhcS0WIiWiwfcymAbUS0GcCTAK4QSVD45MZZwwAApxRkBOxbIa9L2uLy6Lb/5NWN+LEc3FtcHuyvacWd/94S45YyxlgY1RaFEKsRfL1l5ZinADwVrUb1FD87dyRunT8CPiGQYrXgoe+Ox8G6Nvzps73qMXuqm9XHmw83qKmXJ6+YpM4yVYZAMsZYLPW68rmRICLYrVIw3vPA+QCA577Yr+5PT7Fi6xH/gtNbK/yP69s61ZSN1dJrJ+Qyxk4iDugRStcU9cpOtePQ8Tb1uZJXB4A9Vc34r5w7t3I8Z4ydBBxqIpTusKqPlZuninc3HYWSXbnuH9/gxa8PAoBuZSSvT+Dhj3ehtiWy8evNrk40uTq7PpAx1mtxQI9Qeoq/h55uKMFb0+xG/+xUpFgtuiqNVqs/oK8uq8VfVu7DPe9tj+h9J9y3FBPuXdrNVjPGegMO6BHSBvE0Qw8dAK6bWYyfnD1ct63V7UWLPKZdWTxDeR6uxB8zxBiLNQ7oEdKmXLS9dQBItVtx45nDcPPc4fjmV/PV7cdbO3DGg59JT+TOuo8jNGMsyjigR0jbQ1dy6AWZUkGvTKe0j4jUbYomlwdLvi6HOwqLZPh8Ag9+tBP7a1pO+FyMseTBAT1C2qXrlMUvhuZJtdWzUu0hX3vv+9tRLRfzOpEe+u6qZvzti/34vzc2d/scjLHkwwE9Qtq8+YSB2QCAq6YPBhB6WTsA8Alglbx4hs8H/ObdbVi0pDTiNigjZFLtgTl8xljvxePQI6TkzX94RjFumz8Si848BZlOG/bXtOIH04d0+foVu6T66j4h8NKag7p9Pp9AS4cHWc7QPX2lhG9uRgp++tpGfLWvDus0OXvGWO/EAT1CFgth9+8WwG6xwGIhZKdJX3JuO2dkWK/3yKNcOr0+dZvXJ2C1EB5btgdPrSjDlnvPDRnUj8o12nPTUtQPBSEEiLjEAGO9GadcusFhs8ISRn2W568pCbqvWVPU60BtKx7+eBdeXisF54ZW/QQiY831inopoGvz8MowSFenF0lQF40x1g0c0GNo/phCzBiWpz532Py/7r3V/hEq8x/7HH9ZuQ8NbVIgb3J14v9e34S73t4Cr0/g9x/t0p23Sl4GT7uaUlWTG82uToz+zcd48rOymFwPY6xn44AeY9rUyvgB2WG9prbFjbc3VuDVdYdxpL5Nt+/N0sOoa+kAAGzRFAarbnLhzdIjAIBPth8Lef5dx5pQfOeH+KqsNqz2MMYSAwf0GNMG9EG5aabHpNj0/xmUG6eAVLVR6/a3tuBArVQE7Hhrh7q9qtmFvXIp34F9UrGtohHPr9oPM1+WSWuPLNtZFe5lMMYSAAf0GOvw+vPZVguhKNuJsUVZam/9b1efhj2/Ox8/OmuYepxS1AsA6jVBW6EtG9AvSxoqWdXkhqtT+vCob+vAt55ajd99uFMtNaDlkT9k7FwGkrGkwv9Hx9jwvv7Vjobmp2PVL+bhgx/PQl5GCgB/jZainFTT1x83Ceha04bmIsNhQ1WTC+0d0izUutYO9bzNrsCaMcpIm64W3vhsZxXO/uNK3bcMxljPxcMWY+yhS8bj8pJBcHu8mDOqrxpEb5k7HCt312Dy4BwAwQN6fVvogJ6XkYK+mQ5UN7vh8kgBXdur31fbgqLsVPTTTHryyN8a7F0E9Dvf3oqaZjeOt3agMCv0pCnGWPxxQI+xdIcNs0bkB2wvKc5F+UMXqM+DzTL93Yc7TbdbLQSvT6BPWgr6ZjlQXtuqjqJpaPfn3S/5y1cAoHsvj88nn0M6/o1vDuPZVfvx1uIZyElLCXgv7qEzlhg45dJDdFU2wKivXPwrJ82Owiwnth9twoZDDQC6LrXb4ZECtFc+8O2NR1BW3YI5j65EnWbhDaX/ruTmGWM9W5cBnYgGEdEKItpBRNuJ6FaTY4iIniSiMiLaQkRTYtPc5JWX4cBri6bjupnFIY/Lz0jBV3fOwwA5RZOTlqJbEamryaIer08dA++SKz8q5Qwa2jpR2egKOJeSm2eM9Wzh9NA9AH4mhBgDYDqAm4lojOGY8wGMkH8WAXgmqq3sJaYPy8M93xob8hirhVCUk6quiNQnza6btXrmiIKQQf3xT/dguTwsUgnotZqcu1l6RXmvL8tqsf5gfXgXwxg76boM6EKISiHEBvlxM4CdAAYYDrsYwBIhWQMgh4j6R721TO2N+wN6Cu46f7S6vzDTgbFFWUFfv3qvfzKROiqmxa3Wb1fSMQBA0L/XVc+vxXef+Soal8EYi4GIcuhEVAxgMoC1hl0DABzWPD+CwKAPIlpERKVEVFpTUxNhUxngX5/UJQfj7FQ78jIcuOp0qYSv027FmP7BA3pehn/hDZccvI+3dqg5/E7NuHmlp//8qv26QO81GdvOGIu/sAM6EWUA+DeAnwohmrrzZkKIZ4UQJUKIkoKCgu6cold49abpuP9ifeplwdh+APw99JLiXABAdppUlTFDXi0pNcWKgX0CZ6QqqRRtPry9w4u2Dg/aOrxqQO/wBubLV+2txYtflavPT/nlRwHH1La4uyw5wBiLrbACOhHZIQXzV4QQb5scUgFgkOb5QHkb64YZp+ThmhnFum3XzJBqrSvj2B++dAKW3naWWmZXubFptZA6e1RLyZdXNbtQnJeG3PQUuD1etS5M/2zpJqu2J67V7Oo03a54Zc0hLH55vfo+jLGTL5xRLgTgBQA7hRCPBTnsfQDXyKNdpgNoFEJURrGdvZ6yfqlNHjvutFsxsjBT3a9M4/d4fZiuqfCoUIYeVje5MXd0X4zul4n2Di/2VEn1X0YUSjNaF7+8AduPSkW/tPdWjfVmjI42tEMIwM1DHBmLm3B66DMBXA1gHhFtkn8WEtFiIlosH/MRgP0AygA8B+B/Y9Pc3ksJ5MGm69vl3HqnV2BwXhr2PnA+5p9aqO5/b1MFvtpXixa3B/2ynEi1W+HyeLH+YD1sFsJUOYUDAL98eysA6BbMMNZ98RhGwxxrkoY7KrNVGWMnX5czRYUQq6HvrJkdIwDcHK1GMb3/3nom3B5ldqf5fwqbvF25YWm3WpCtWbRaO+N0SF46thxpRHuHF5sON2BMUZZuhaQWtwcPf7xLV2/dbUjFuD0+2DRB/pg8fp176IzFD0/978G+uH0urFbCgJxUrD94HECIgK6kXDQjULQBXWtofjqcditcnT5UNrowtihL7eEDwL6aVvxlpX6VpFa3vsiX2+NDun/ADPfQGesBOKD3YIPz/KNV8uToefrQXNNjU+1Kjt0fmHPSAgM6ETAkLw35mSmoaXbDaiHMGVXQZY68JSCg60fLNMr1Y7iHzlj8cEBPEMX56fjkp2fhlIJ00/3fmliE3VXNuHnOcHWbWQ+9f5YTTrsVJUNy8bfP9wNeoCDT0WVt9CZDGV5t4FZ654A+0DPGTi4O6AlkVL/MoPtSbBb8cuGpum1mAV2ZWDS1uI+6rSDDgZQuAnqDoYyvNqd+TFP/hQt5MRY/XG0xiU0Z3Ccg7aIE+Zy0FIyShz3mZzp09WC0lt52FkqG9Amoy77xkL+mS1WIHvqa/XWY8ttlaGzvhBACxxpd+Kb8OO56eytEV2UhGWMR4YCexAbnpeE/t8zSbdP22qfJ+fgCTTkArcWzT8HIwkykO2yob9VPLLrz7a3qePVKkx66Eqy3VTTieGsHqppceHP9EUx/8DNc9tev8eq6Q9ybZyzKOKAnuT7p+gUrsjQB/YIJ/TE0Px1D8swXr85wWOV/bQEpFwCobJACubGH/tiyPZh0/zK0d3jVJfT+9OleLDWUBmh2d+KfXx7QDY9kjHUfB/Qkl+GwYft95+H280YBAJx2/3/y6cPysOLnc5DpNB/emCaXExjYJxWtcg2Yq6cPUfcLSOPeNx1uQJZcS6bZ5cGTn+1FY3ununwdAHy4tRKf7qzWnX/PsRbc+58duPmVDabvL4TgtAxjEeCA3gukO2ywyLM+uxrNoqVMFL1u5lB1m1IEDJDqw3xZVotNhxuw6KxhAPQjXhraO1AXYpHrg8dbAQRfQOOcx7/AjAeX67Zd+OdV+NU7W+Hx+nCgtjXsazEz/7HP8fyq/Sd0DsZ6Eg7ovYRXXkfU1sXC0FpK51i7wHSmJqD/celuXPP3dQCABeOkapDa9Etje6faQzezr1oKyNoPiXc2HsHH26QyQGXVLboPCADYVtGEV9Yewu8+3Im5j65EdbO0/0BtK8ojDPBl1S1B12xlLBFxQO8llDrn4QR0bdAO2Ofw7yuva1MfK9Uaa5r9a5I2tIUO6GU10lJ4GZpz3vb6Zix+WZ+CuflfGwJSLx9vk/LxjW3Szdq5j67EnEdXBn0vo1ilch74cAem//6zmJybsa5wQO8llJmgGSGCteLBS8YDAE41WSgj2OvTHTakWC0BPXTtotMK5UNln7y2qTavb+bDLZU4dLxNt02ZuarMUI2UdiGPaHpu1YGAbxWMnSw8saiXuH7mULR1eALqrJu5cEIRpgzugyJ5IWqtDIf5DVQA6PD6sKeqRX3+63e3BRxz9fQhuHX+CJT87lN1dEu7PHzRWC9Ga+OhBgzO9Y/GUQJ6Q1v3AjrPaGXJiHvovURqihW3nzcaTrnmS1fMgjkApDvCe30wc0YV6FIsAPDFnho88OEOTH3gU3WbcaGNDYfq0WGygHVDN3vowRbyUNz7/nbMeWRFt84NxC6lw1goHNCZKtcwZl1LSYsoQxm7Y3S/TMweWQCHzaKr7ghIqYo2zWiXkb/+r27/xkMNphORGto6ulzjVAgR0CM3+3BQfLilEv/8qlx3j0DrUF0bLn3mKzV/byZWKR3GQuGAzlRf3TkPO+9fYLpP6dmHqvnyyKUTQp7/45+eBZvVAiJCuiOyD4adlU26AKp8wDS2d6LFFTxVAwBPryjDqF9/rFtGL1QP/eZ/mY+LVzy5fC9KD9bj4+3BF+UyfoDsrGzC0yvKQp6XsRPFAZ2pnHarutSdkUO+qZpiI5Q9cD5W3TEXU4v7YHS/TDz2vYkAgMtK/MvKLr3trJDvlZsW+G3AbiWcNdJ88XCPT+D51f4x4+MHZKNPmh0NbZ1o0gRqs1THq+sOA9Dn240Ldvi36wOxcWUm6T2kf7UrOhkp3yaeWr4Xa/fX4eKnv8Qjn+xWF+tmLBb4pigLi9JDtxDBZrVgUG4a3lx8RsBxX945Dw1tHbr1TmePLEDfTH29mL5ZDuw3jBsfNyAbT1w+CVN+u0y3/cwR+XB1erHk64PqtqH56aht6UBDeyeaNT30JpcHGQ6bbiEQJchrg7ixh/7Ep3swfkA2VpfV6ra3dXqRZfhWopwv1ABQ5YPh0aV7dNs7PL6IJncxFgkO6CwsSg89VO4ZAAbkpGKA4Ybq3384NWClpZzUwB56XroDuekp2PCbc9SgPrIwA49cOhF/XLob35RLFR5tFsLt543GjS9+g6Z2fQ994n1LkZ5ixTVnFOMXC0YDAJQUe1uHP/Brg/vCP63Cjsom0+tpdXt0y/M98OEOvL2xAgDU2bdm3B6f6bcFV6c34nST0b6aFhyoacX8MYVdH8x6Fe4qsLA88J3xGDcgC8V55gtsmFGGGZotm6dUfRw3wD/WPU1O9/TRlPy976Jx6Jft1AXBF6+fhoJMB9IdNrR1eHQ9dABo7fDimZX7UC2PBxeQAmur259O0fbQgwVz6TX6cz+36kDQY7VcnV7TtE6wVE8kLv/bGty4pBSuTh56yfS6DOhE9HciqiaiwEHF0v45RNRIRJvkn7uj30wWb1OLc/HBj88Me9gjAPznlllY+fM5pvuuniEV+Xrqyil44DvjAPgnPxGRZlSN9H7a2avaETfflNfj1tc2mr7H8l1SMTBh2kMPLxi2ur34y8oynPf4FwH7jN9Wlu2o0pzfZ7ocXzQCupKH33Kk8YTPxZJLOD30fwIwH/rgt0oIMUn+uf/Em8WSQXaaHcX55j36cQOyUf7QBSjOT1eHHWrXNVWW0lMW6ND20B02payv9G+bobiX8prjcslfJeXS2mHeQzca2MefMmp1e/Dwx7uxu6o54IamsYd805JS9bG702e6YHY0JjRNGJgNACiVFw5nTNFlQBdCfAGA/3JYzChj2/tl+YuA3TJvOFbdMRdD5BSPdjKSMhInzSQXPSAnFRt/cw5sFtIMZ5Qiepvbg8a2Tni8vpD3ArQ3cLUfAtXN+jIGoRbocHm8plUko7GItrIgeHfLHrDkFa2bojOIaDOAowB+LoTYbnYQES0CsAgABg8eHKW3Zonu25OK0OLqxJWn+/8miAiDNFP9tQFdSfsYZ5wCUmqGiJDhtKm5daWHXt/WiYn3L8W1M4Zg4qCcoO0p0AT03cf8+fVKw0Ic2h668QZo8B76iQd05RydnuhOXtpypAHjB2SHHI7JerZo3BTdAGCIEGIigD8DeDfYgUKIZ4UQJUKIkoIC8/HGrPexWS344cyhairFjC6g2/T5dauF8MTlkwD4c+2ZTps6kUgJts/Jtc8/3HosZGDtm+n/pqAddnjpX7/WHfenz/aiTC4w9sVe/XBHt8er9uB/JNeKB4DX1h3Cnz/bG/S9w6Gkizq8+g+M6iYXfF3Mmg3mo62VuOipL/H+5qMn1DYWXycc0IUQTUKIFvnxRwDsRJR/wi1jTCM9RA/97NF9cfowaX3UH8grKmU67GoPXQneSinf2hY37np7a9D3CqcipeKaF9aiptmNa+W68Ap3p0/twWu/Dby9sQJ/XKYfmx4pJV2k7aFXNrZj2u8/wxOf7sEn249FXEtmT1UzAJzwoiEsvk44oBNRP5K/oxHRNPmcdSd6Xsa0tKNclJ65knu3WQn9s1Nx4MGFuHjSAPX4ZpcHXp8IuGnalWDlDYpN1l492ujC1oqGgO1SD11638IsZ5czZwHpm0RlY9frqyo9dO1N2qNyOujJ5WX40Uvr8Z8twcsSmPHItWesRPhqXy02H26I6PWsZwhn2OKrAL4GMIqIjhDRDUS0mIgWy4dcCmCbnEN/EsAVgkvNsSjT9tCVHG9qivTnq0zw0eZ+M502NLs9+PNyfXojPyN4ATKFcr5zNRN33r15JlbePhcv33B6wPHLdlQHbHN7/D30VLtVnZhl5vM9NVh34DhW7q7BrD+sUGvKP79qP+54a3PA8f6Ui3bmq/5/ueoQNdnrWtzqSk8Kj5yqsVoJ339uLS5++sugr2c9V5ffLYUQV3ax/ykAT0WtRYyZUMr2ahfdULoNZqswZTrt2Fddi7/UtOBbE4vwHzk3PGlQTsBi1YpXb5qOU/qm4zW59ou2+mRRjpRXnzk8L/B16w4FbHN1+nPoTrsl5P0BJV1z94Vj4PUJ1DS7UZjlVJfHe+A743XlAtSUizagG0bt1MtDNjs80oge7T2Ikgc+hRBA+UMXqNuUJQrtFp5rmMj4vx5LCH0znXjgO+Ow5Ppp6jalV2kxDeg2dHh96PD48KOzhuHz2+dg3S/PDlnWtjDLobshmqMpIJafLo18CXcEiEuTQ3ea9NCVm5fa4l91rdKwyHbD+PZdlVJ+u8nViaXbj2lSLv5rae/Qz2g9dFxKwXz/uTUYd88nun3KB6H2i7TSDLPfJUscHNBZwrjq9CG6IYXK41MKMgKO1ebchxWkY0heOvpmOdVerdmQRyUnr4Q0bSpdG+jW/vLsgNe+e/NM3fMWt0cf0A3L7Ck3aisb/amPuhapV63k/JX00NYKaUboz97YjEUvrVdXetL20Fvc+g8BZcm+0oP1AW1VVGiGYXpMFhE3qzR5onw+gdmPrMA7G49E/dyMAzpLYHNGFuAf103F4tmnBOzTLlenXZRDCYJ/u/o07H3gfGy77zx1nzJhR+mEB7sTVKiZAJXhsGH+qYWYNCgHt8wdrm5vcXvUpfWkHLo+5eLq9MLnEzjzYf+qSMpapMqEJGU0j1JPxjgCRTv0ssWln2TU0KZfnFvpjWsXA9lb7V8u0GMy3HH4r/4bsO1EdXh9OFjXhp+9EXhvgJ04rrbIEhYRYe6ovqb7pg8LzHUDwHlj++Gb8noU56fDbrXoctPOlMD+zc/PHYlMZ+A6qjfMGoqK+nY8fdUUtUev7cW3uPw9dIfNAouFYLeSmiapa3XjZ2/qg9peeT3W9k4pgCsfKG6PF9XNLpQbAnqn14e2Dg/qWjrUNVYVxrIElz+7Bv2ynLjnW2PUbZUN/m8HXrldxtfVNLsx9YFP8eL10zA7SK36SCgfqMGGyz+2dDeONrrw6GUTT/i9eiMO6CwpKT30M0fop0TcMGsoLj1toC4/ftv8kXj80z0BwxUFgFvmjTA9/28uHBOwzarJrx9tbEduRgpS5GAOAHarBZ3yZKA3S4+oxcPmn9oXn+6s9i+a3eHDS1+Xq8/dHh9mPbQioBe98VADxtwt5ccXaSYvSefw4vK/+SdCrTsgVe+4ZZ7/W4S2WJlybmPlym/Kpdf988sDUQnoXa3l+uRyaVUnDujdwwGdJSUiwpZ7zw0I0kSkC+YAcOv8Ebh1/gjdMd2hfastRxqx5UgjhgUpTubT5HMWjOuvG3nzy3f0k57cntC1ZwDg2S/8qzmlWC1wdfqw9kBgCabaFn89Gu34fKU9zYbUjVIvxqwEsuLjbZUoLa/Hr00+5Ix4rdXY4hw6S1pZTntE5X4V18wYgm9PKtJN2Q+H2QiREYX+G7baHvy7m/xT7CfK1RODcUdY9zw3PSXoB0BNs3lAV3rOTYYeeq18fKjFPBa/vAHPrz4Q1uzUcJfg66qEwR+X7saXhtWluuPw8TZsOdJwwufpKTigM2aQ6bTjiSsmB/Tku2I1CXp5Gf5ROVmp/ly8NrD2y3YilBc1S++FQzt+3uiwPPrFYbPoUi7KUMkmQwXHw/XS8WU1Lahv1d9oNQqn+mO4xclW7qlG8Z0fms5YFULgz8vLcNXza8M6VyhnPrwCFz0V+SSqHUeb8PG2yGbjngwc0BmLEmNaIstpw8/PHaU+75MeeHN1233nIcNhCxinnhciKGuZlSkIFdC3H21CQaYD+RkOXQ9dGVmzYrd+0tVheTz7/ppWnPP45yHbcizE7FQAGH/vJ1j0UmnIYxTKwt5f7w+sItLU7gnYFqmPtlZi9iP+EUaRTm5f+OQqLH55AwBpScAPtvSMomYc0BmLEmNA/+23x+mCq9k6qhkOqdxvfoZxEe3QvXalns2FE/ur25T36hMioG850oghuWlITbHqeuhKqV9jpkPpoQNAbUsHdhxtwlPLzatFHmsMHtA9Xh+aXR7srwmv+JdyLqdJyQRlApbi3Mc/x0P/3RXWeRV3vb0VB+v81xas3o8Qosv0z6XPfIVb/rUx7HRSLHFAZyxKtGUJACA9RT/mICctsIeuyM/UB/Suas4oAX1gH/94e2WMeajefUVDOwbnpSE9xapbY9VsMQ4AOFKvLxa28MlVeHTpHtOqjDsrmzHv0ZXYaVij1esTuOHF8HrmCqW37zC5B1Inp34y5clhe6pa8NfP90V0fmO5CGVSl9F5T3yBifcvDXkuZWSQMpkrnjigMxYl04flYfUv5uKGWUMBAP1z9L3sUYWZQV9bYAjgoYp5Af6ROFlOG0YVZuKOBaPUnmSfLnL/A3NSkZYi1YtXSu02tEW2+tHcR1fiiz01APwfLku+Lsf+2lY8s1IfXMuqW/C5fKxWp9eHg3WtAQtxA/57DGa93jp5pE6G09bt+u82qz6g76lqxrmPf4573tuGXZpFTfZUtQQM5TRSJpodqDFO/PIGzB2INQ7ojEXRwD5puGPBKLx603SMLdKPXvmfOafgoUvGA5BquH/yU39J3fwMhy6I20IUyTpzRL76bSA1xYpPbjsL/ztnuDr0MNckV69VmO1EWooVGw414EcvrcfLaw52mf82s2pvDTxen3pDVSljkG4oq7Cj0nwx6wueXIXZj6zEzf+SctFmwVv5FvHepgr84q0tAPw99AyHDS0d3cunG3+///jqAPZUteDFrw/i4ghukgoh0F++qb2/tkW37xdvbcGcR1eafmDFCgd0xqLMYbNiximBM1VtVguumDYY5Q9dgBd+OBWj+vl77NfMKMbvvj0Os0cWYFpxbkAPUpGf4cBLN5yOfDmt4tSUFFA6q7np+vTNd6cMxIbfnKM+75vp1K3HunRHVVjXlWVY+OO5VQcw/Ff/hRBAtmYEj8NmgRACb60/gsb2Tuw42gSrhQKW/dsjz4xdubsGv3xna8AsVcBfdOzW1zbh9dLDWLr9GN5aL9WBSXPYAkblhMt4v0ObWopkmUCvT6izjY826D8Uld+rNj9/oLY14huwkeCAzlgPMKYoC5eVDMKL10/DG4tn6EoSAMDT35+CxbNPwUs3SNUmlXHmdk2v3isHCuNomvPH9dPdnC3MciBNk5tetbcWKSYpnsmDc/DVnfPwwY9n4VcLT8XwvoFF0BTfmTxAfdzi9mDN/uP4+Zub8fDHu3Cgtg3DCzIwYUDw8fb/WnsooMokoF+kGwAWvbQeGw81AACONbZjp1yJUlHX4sZVz69Rh2d6fQJuzdquL31djtLy4wE3QbU3SCPR6RVok9ttvA+hjO1XPqi+KT+OuY+uxOvfHO7We4WDAzpjPZDxpl2aw4o7zx+tplqU9ESKpiffT87laoP357fPwXzNQh2AlPNNc+hvNk4rzlUf3yjfAxicm4ainFSMG5CNm84ahlH99Dd9tQbkpGKIvKJTfWsHSuWSAY3tnWhydSI7zR7wnkZlVS0B20KtNlXV5MZNS/Q3W1eX1eLLsjp1tu0Tn+7BmLs/UUsf/Oa97bj0r1+rM2aH5acHVN6MpIJwp8+nfotoM3wgKeUUlGtQbhYr1TNjgQM6Yz3QZSWDdM+NI2aUKfTaXPCri6bjse9N1KU/tFUnFXnpKQE3Ts8a6a95kyrf5DTODp08OCdoe8cWZeGvPzgN/bKcqG/rwDo5oDe5PGh2eZDltCHNHlhp5Jwxhfjgx7MAACtNbpy2RZgjV2a6bjnSiPYOL579Yj+8PoGPtlYGpDpuP28UPv2/2RhqKM9gC7IEISBNzHp6RZn6vNPjUwO2sSa98Rr8xdoin70cLg7ojPVA04bmovyhC9RetzKSRKH00LW59gE5qbhkykBdXl1bl+b5a0pwzYwhsFktuOr0wbpe+WWn+T9AlHIJxo5qyZA+Ae38xYLR2HH/eThjuHSjdurQXNS3daJCzkmX17ai2dWJLKddXXUKAFbdMRfXzSzG778zHmP6Z2FkYYauHo3ivU1H8ffVBxBueZ1KuaCZTwhc+dwaNR9e0+LGT17bpDv2+9MGw2IhXTlkALDLXXTtB4Dy+MYXS/HIJ7vV7esOHFdTLa1u828Tyn63XE7ZWBs/mrg4F2M9mPK13RjQv1cyCKv21gaMfQekG5QXTSzCtWcM0W2fP6ZQTb/kZTjwxuIZ2FPVjMb2Tt1kJH9deH0UHVaQgReuLdGNKZ9a3EdXbz43zY7qJpf6DeJIfRucdisynTa15w9II1Tu+dZY9fmCcf2xp0qasDR5cI6aJweAf35VjlS7NazFvpUJSc0uDzYdbsDIwgx0egU+NCyafc6YQvWa6w2145UeuvbmaIfXB4fNqituBgD/88oG9bEx5aJuV3rw8n6zEhHRwj10xnowZa1P41DAb00sQvlDFwT0LgGpSNiTV07GaUNyA/YZjSzMxNRi/XHKCJ3zxhYGHG8ciplmSAVNGdIHrR1edHh9mDAwGz4hBbRMp12XNrIbbsJqR9D8cuGp2K5ZeKRvpiOskSdCCBxt1E+EevCS8aYTrbQrWl05bbBuX2N7J4rv/FD3IaC8f6jyv8FSLkogr5fH+ofzwdRdXQZ0Ivo7EVUT0bYg+4mIniSiMiLaQkRTot9MxnonZfZnVxONounU/lnY+8D5OHdsv4B9qYZvCsZvDvNG+xccOeMUf14+K1XfQzfWoNHemExPsek+wFJTrLqVlhaO74fvny4F4R/NHobrZhYDkEbE7D6mH/UyojATGc7ARESWZtGSS08biAMPLsTri6br7j+8tMZfFG3FrmqUVbeELGOsDdTaIZhKyuW4XLIg0vsCkQjnr+SfABaE2H8+gBHyzyIAz5x4sxhjAPCP66bi25OKdAHoZDAOm1SkGqbiG0euZDrt6qIis4bn67breuiGcfbaoKuUSPjhGcUApGGV0jmkY+46/1T195HltKvlDx7+eBfq2zpx4QR/fZtMh810/dhMQ5AnIpw+LA+3aeria8eq3/raJsx/7POQ9VqO1LfjiFz7pk5TmfKOf2/BwbpWHJe3GVeXiqYuA7oQ4gsAgZXy/S4GsERI1gDIIaL+IY5njIXptCG5eOKKyaa11qNNG4CDMQZi4+gbAHj26hI89f3JmDncP7kq02nT1bIx5ue1y/wpveR7Lxqr6/HfNn8klt12FgblpqmB1W4l9UNmydcHkZ5ixcLx/vBDRAHBW2mPGe29BLNfebBKA0paZ9YfpAqOxwypnzdLj6hj3WOZconGTdEBALQj5Y/I2wKKBRPRIki9eAwePNi4mzEWR0uun4au5jAaA7Gxxw5IKZILJxQBkCYxVTW5kZ5iU8epm9H2orVpHO3QyVkj8jFCrofjD+gWpGrWgs3NSAkogma2ApVZ2WEAmKSZzRpJfZv8DIfaK99xtAnffeZr3f7G9k5Uy/Vp4tpDjyYhxLNCiBIhRElBwYmvT8gYix6LhUIuNRfsNaH8WF6TtSDTYbrYtkLbY9YG4Ba3FFSf+v5kjNQUN1PH4VstaO/wp0Ee+94kZDr076MMF9QKtsygdtx+VQT1bfpm+cstvLe5ImD/xsP1AACn3RL3HHpXKgBoZ0EMlLcxxpKQceHtUH4wfQi+vHMexoWY9g/ANM8N+HuzRTmpuu0epYduITUN9NAl4zG1OFc33h2Abuq/ItjnEBGpC1Qbl+MLRfsB4TDp/W+rkGaJTi3ODTpePRqiEdDfB3CNPNplOoBGIUTPW5uJMRYVL91wekTHD9AE42CzTc1GogDATHmkzMA+hoDu8/fQL5kyEP/44VRcPnWQ6bm8Jonv/tmpAdsUl542ELNHBmYQbps/EtfOGGLyCv0HxL4gi3iM6JuBATmpONrQHrMKjF3m0InoVQBzAOQT0REA9wCwA4AQ4q8APgKwEEAZgDYA18WkpYyxHuPf/zMD1U3urg80eONHM0wDrNnNVUCann/1jCHom6kfb//dKQPxzsYKnD40F1YLYa7m5qmxt//rC8eoQybf3lCBJddP6/Jbhnb4ouJWeQTMm+uPBNzYvO+isZj9yEoAwI7KJuSmp+DF66bh4PFW3Pv+dtS2dKCkOBfzT+2LN9cfwQMf7cTvvzM+ZBu6o8uALoS4sov9AsDNUWsRY6zHC2fSkhm71QKT+6hBc/c2q0W3KpNi1oh8lD90gelrUu1WXDC+v9pjH5CTise+Nwk+n8CDl4wPq5aKcrP31P5ZASswvX/LTLyw+oC67ikADMlLx19/MAWLX96AA7WtuHBCf4wfmI3xA7OR6bTjzdLDuHr6EIwpysKS66dhjMkM32jgqf+MsR7h9KG5OGdM4OzUSBERnr4qcH6jxUJwWMIrjDV7VAF2HWvCzXOHY9FL6+HU1F8Z3jcT/ztnuC6gA0CWpldfoFlScPbIAl0KZ2YYw0O7iwM6Y6xHeP1HM+LdBNXC8f2xcHx/HKyT8uHGHrVZ/Xhtmia3i2UAY4UDOmOMBTE4Nw2/vuBUXDxpgG67dibtv26UbhJrSxL3CbFQdyxxQGeMsSCICDeeOSxgu3bG7BlyCkWbZulqoe5Y4WqLjDEWIbNaN9ptxmUATxYO6IwxFqFgxcsU8eqhc8qFMcYiZLUQFs8+BQvGBZYYBvTrup5MHNAZY6wb7jx/dMA2h80Ct8cXUCDsZOGAzhhjUfLuzTPxxZ6amC4EHQoHdMYYi5JT+2eZrvN6svBNUcYYSxIc0BljLElwQGeMsSTBAZ0xxpIEB3TGGEsSHNAZYyxJcEBnjLEkwQGdMcaSBEkryMXhjYlqABzs5svzAdRGsTmJgK+5d+Br7h1O5JqHCCECV7FGHAP6iSCiUiFESbzbcTLxNfcOfM29Q6yumVMujDGWJDigM8ZYkkjUgP5svBsQB3zNvQNfc+8Qk2tOyBw6Y4yxQInaQ2eMMWbAAZ0xxpJEwgV0IlpARLuJqIyI7ox3e6KFiP5ORNVEtE2zLZeIlhHRXvnfPvJ2IqIn5d/BFiKaEr+Wdx8RDSKiFUS0g4i2E9Gt8vakvW4ichLROiLaLF/zffL2oUS0Vr6214koRd7ukJ+XyfuL43oB3UREViLaSEQfyM+T+noBgIjKiWgrEW0iolJ5W0z/thMqoBORFcDTAM4HMAbAlUQ0Jr6tipp/Alhg2HYngM+EECMAfCY/B6TrHyH/LALwzElqY7R5APxMCDEGwHQAN8v/PZP5ut0A5gkhJgKYBGABEU0H8AcAjwshhgOoB3CDfPwNAOrl7Y/LxyWiWwHs1DxP9utVzBVCTNKMOY/t37YQImF+AMwA8Inm+V0A7op3u6J4fcUAtmme7wbQX37cH8Bu+fHfAFxpdlwi/wB4D8A5veW6AaQB2ADgdEizBm3ydvXvHMAnAGbIj23ycRTvtkd4nQPl4DUPwAcAKJmvV3Pd5QDyDdti+redUD10AAMAHNY8PyJvS1aFQohK+fExAIXy46T7PchfrScDWIskv245/bAJQDWAZQD2AWgQQnjkQ7TXpV6zvL8RQN5JbfCJewLAHQB88vM8JPf1KgSApUS0nogWydti+rfNi0QnCCGEIKKkHGNKRBkA/g3gp0KIJiJS9yXjdQshvAAmEVEOgHcAjI5vi2KHiC4EUC2EWE9Ec+LcnJNtlhCigoj6AlhGRLu0O2Pxt51oPfQKAIM0zwfK25JVFRH1BwD532p5e9L8HojIDimYvyKEeFvenPTXDQBCiAYAKyClHHKISOlgaa9LvWZ5fzaAupPb0hMyE8BFRFQO4DVIaZc/IXmvVyWEqJD/rYb0wT0NMf7bTrSA/g2AEfId8hQAVwB4P85tiqX3AVwrP74WUo5Z2X6NfGd8OoBGzde4hEFSV/wFADuFEI9pdiXtdRNRgdwzBxGlQrpnsBNSYL9UPsx4zcrv4lIAy4WcZE0EQoi7hBADhRDFkP5/XS6EuApJer0KIkonokzlMYBzAWxDrP+2433joBs3GhYC2AMp7/ireLcnitf1KoBKAJ2Q8mc3QModfgZgL4BPAeTKxxKk0T77AGwFUBLv9nfzmmdByjNuAbBJ/lmYzNcNYAKAjfI1bwNwt7x9GIB1AMoAvAnAIW93ys/L5P3D4n0NJ3DtcwB80BuuV76+zfLPdiVWxfpvm6f+M8ZYkki0lAtjjLEgOKAzxliS4IDOGGNJggM6Y4wlCQ7ojDGWJDigM8ZYkuCAzhhjSeL/AeV+BUNJ4SRyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#################################################\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrccVZ1gDny0"
   },
   "source": [
    "### *References*\n",
    "[1] [practical pytorch](https://github.com/spro/practical-pytorch)(https://github.com/spro/practical-pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 실험 내용은 https://velog.io/@jooh95/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-Attention%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-Seq2Seq%EB%A1%9C-%EA%B8%B0%EA%B3%84%EB%B2%88%EC%97%AD-%EA%B5%AC%ED%98%84\n",
    "을 주로 참고하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 실험은 NLP의 기본이 되는 모델인 기계번역에 처음으로 attention을 도입한 논문 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE을 참고사마 조금 변형된 구조를 구현하는 것이다.\n",
    "Seq2seq 논문에서 제시한 모델은 input sequence의 정보를 담은 context\n",
    "vector을 만들 때 fixed length vector라는 한계점이 존재했다. 즉 가변 길이\n",
    "의 sequence가 들어왔음에도 이를 fixed length vector로 표현했다는 것이\n",
    "다. 이렇게 되면 sentence의 길이가 길어질수록 성능 저하가 현저히 발생\n",
    "한다. 이 논문에서는 이 문제를 해결한 새로운 모델을 제시한다. RNN 구조를 따르고 있는데 RNN 구조는 GRU 구조를 사용하였다. 뿐만 아니라 attention 구조를 사용해서 어떤 단어를 보고 예측한 단어를 추출해내는지 결정한다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예상했던 점 : GRU는 LSTM에 비해 parameter 갯수가 더 작기 때문에 더 효율적으로 코드 구현이 가능하다. paramter 수가 작은것에 비해 성능은 비슷하다.\n",
    "    \n",
    "예상과 달랐던 점 및 개선할 점 : \n",
    "이 코드는 전체적으로 기계번역을 위한 함수가 많았기 때문에 아직 정확한 코드들이 이해가 되지 않았다. 예를 들면 normalizeString 같은 코드는 String에서 학습하기 어려운 단어들을 바꿔주는 것 같긴한데 왜 이렇게 구현했는지 이해가 되지 않았다는 점이 아쉽다. \n",
    "개선할 점으로는 attention 논문은 forward backward에서 얻은 hidden state를 cat해주는데 여기는 이 과정이 없다. 논문에서는 이 과정에서 문장의 앞만 이아니라 뒤의 단어도 보면서 예측하기 때문에 더 정확하다고 소개하기 때문에 이런 구현을 넣으면 성능이 더 올라갈 것이다.\n",
    "\n",
    "결론 : GRU를 사용한 기계번역 task에서 사용할 수 있는 모델을 구현해봄으로써 모델의 전체적인 구조를 학습할 수 있었다. \n",
    "    \n",
    "의문점 : 수업시간에 GPT-3와 transformer 를 소개해주셨는데 이해가 되지 않았다. 이 두 모델 모두 long-term dependency 문제를 완벽히 해결한 모델이라고\n",
    "소개해주셨는데 다음에 기회가 된다면 논문을 읽고 어떤 구현을 사용했는지 알고 싶다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 bidirectional  을 구현해볼 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 points\n",
    "class AttnDecoderRNN_bi(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN_bi, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_dim, self.hidden_dim)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "\n",
    "        self.attention = nn.Linear(self.hidden_dim * 3, MAX_LENGTH) # MAX_LENGTH = 10\n",
    "        self.attention_combine = nn.Linear(self.hidden_dim * 2, self.hidden_dim*2)\n",
    "        \n",
    "        self.gru = GRU(self.hidden_dim*2, self.hidden_dim*2)\n",
    "\n",
    "        self.out = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "    def forward(self, input, hn, encoder_outputs):\n",
    "        input = self.embedding(input).view(1, 1, -1) # 이렇게 해야 1,1,256 이 됨.\n",
    "        input = self.dropout(input)\n",
    "        \n",
    "        embedded = input\n",
    "        attn_weights = F.softmax(\n",
    "            self.attention(torch.cat((embedded[0], hn[0]), 1)), dim=1) # 1,512 -> 1,10 \n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        attn_combine = self.attention_combine(output).unsqueeze(0) # [1,1,256]\n",
    "        output = F.relu(attn_combine)\n",
    "        #print(output.size())\n",
    "        #print(hn.size())\n",
    "        output, hidden = self.gru(output, hn)\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hn\n",
    "\n",
    "    def initHidden(self):\n",
    "        h0 = torch.zeros(1, 1, self.hidden_dim, device=device)\n",
    "        return h0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "encoder_bilinear_forward = EncoderRNN(input_lang.n_words, hidden_dim).to(device)\n",
    "encoder_bilinear_backward = EncoderRNN(input_lang.n_words, hidden_dim).to(device)\n",
    "decoder_bilinear = AttnDecoderRNN(hidden_dim, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "learning_rate=0.01\n",
    "encoder_optimizer_for = optim.SGD(encoder_bilinear_forward.parameters(), lr=learning_rate)\n",
    "encoder_optimizer_back = optim.SGD(encoder_bilinear_backward.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder_bilinear.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bi(sentence):\n",
    "    with torch.no_grad():\n",
    "        # tensor을 문장으로 바꾸자.\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        \n",
    "        #### 값 초기화\n",
    "    \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        encoder_hidden_forward = encoder_bilinear_forward.initHidden()\n",
    "        encoder_hidden_backward = encoder_bilinear_backward.initHidden()\n",
    "        encoder_outputs_forward = torch.zeros(MAX_LENGTH, encoder.hidden_dim, device=device) # 10 = MAX_LENGTH\n",
    "        encoder_outputs_backward = torch.zeros(MAX_LENGTH, encoder.hidden_dim, device=device) # 10 = MAX_LENGTH\n",
    "\n",
    "        #### encoder의 output을 저장\n",
    "        for ei in range(input_length):\n",
    "            encoder_output_forward, encoder_hidden_forward = encoder_bilinear_forward(input_tensor[ei], encoder_hidden_forward)\n",
    "            encoder_outputs_forward[ei] = encoder_output_forward[0,0]  \n",
    "            encoder_output_backward, encoder_hidden_backward = encoder_bilinear_backward(input_tensor[input_length -1 - ei], encoder_hidden_backward)\n",
    "            encoder_outputs_backward[ei] = encoder_output_backward[0,0]\n",
    "#         for i in range(input_length):\n",
    "#             encoder_output, encoder_hidden = encoder_bilinear_forward(input_tensor[i],encoder_hidden)\n",
    "#             encoder_outputs[i] += encoder_output[0, 0]\n",
    "        # 위에처럼 구성하면 encoder의 모든 output을 저장할 수 있다. \n",
    "        \n",
    "        # decoder의 초기값 저장\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        \n",
    "        decoder_hidden = encoder_hidden_forward  + encoder_hidden_backward # 1,1,256\n",
    "        \n",
    "        decoded_words = []\n",
    "        \n",
    "        #############\n",
    "        for di in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden = decoder_bilinear(decoder_input, decoder_hidden, encoder_outputs_forward)\n",
    "            # topk는 지정된 차원을 따라 지정된 텐서의 가장 큰 요소를 반환합니다.\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            # top value = topv , top index = topi를 의미한다.\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>') # <EOS> 츨력하기 싫으면 이 부분만 주석하면 된다.\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "                # decoded_words에 적절한(최대의 확률을 가지는) 단어가 대입 됨. \n",
    "\n",
    "            decoder_input = topi.squeeze().detach() # topidex의 값이 단어로써 들어가게된다.\n",
    "\n",
    "        return decoded_words\n",
    "    \n",
    "def evaluateRandomly_bi():\n",
    "    pair = random.choice(pairs)\n",
    "    print('>', pair[0])\n",
    "    print('=', pair[1])\n",
    "    output_words = evaluate_bi(pair[0])\n",
    "    output_sentence = ' '.join(output_words)\n",
    "    print('<', output_sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> tu es bavard .\n",
      "= you re talkative .\n",
      "< supposed experienced experienced supposed currently supposed experienced supposed currently supposed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************* iter1000 *************************\n",
      "loss 25.9588\n",
      "> je peux courir .\n",
      "= i m able to run .\n",
      "< i m afraid . <EOS>\n",
      "\n",
      "************************* iter2000 *************************\n",
      "loss 45.9657\n",
      "> tu es mon meilleur ami .\n",
      "= you re my best friend .\n",
      "< you re very . <EOS>\n",
      "\n",
      "************************* iter3000 *************************\n",
      "loss 16.5896\n",
      "> elle a les pieds en dedans .\n",
      "= she is pigeon toed .\n",
      "< she is afraid . <EOS>\n",
      "\n",
      "************************* iter4000 *************************\n",
      "loss 17.7468\n",
      "> nous ne sortons pas ensemble .\n",
      "= we re not dating .\n",
      "< we re not a . . . . . .\n",
      "\n",
      "************************* iter5000 *************************\n",
      "loss 27.0172\n",
      "> je vais sortir cet apres midi .\n",
      "= i m going to go out this afternoon .\n",
      "< i m going to . <EOS>\n",
      "\n",
      "************************* iter6000 *************************\n",
      "loss 14.2431\n",
      "> je suis pret a retrousser mes manches .\n",
      "= i m ready to roll up my sleeves .\n",
      "< i am a to . <EOS>\n",
      "\n",
      "************************* iter7000 *************************\n",
      "loss 9.7893\n",
      "> tu en as deja fini .\n",
      "= you re finished already .\n",
      "< you re finished . <EOS>\n",
      "\n",
      "************************* iter8000 *************************\n",
      "loss 25.7795\n",
      "> il enseigne depuis ans .\n",
      "= he s been teaching for years .\n",
      "< he is leaving . <EOS>\n",
      "\n",
      "************************* iter9000 *************************\n",
      "loss 9.4277\n",
      "> elle a honte de ses vieux vetements .\n",
      "= she is ashamed of her old clothes .\n",
      "< she s ashamed . <EOS>\n",
      "\n",
      "************************* iter10000 *************************\n",
      "loss 12.0673\n",
      "> je te donne encore une chance .\n",
      "= i m giving you one more chance .\n",
      "< i m not a . <EOS>\n",
      "\n",
      "************************* iter11000 *************************\n",
      "loss 13.0557\n",
      "> tu n es pas aussi malin que moi .\n",
      "= you re not as smart as me .\n",
      "< you are not me . <EOS>\n",
      "\n",
      "************************* iter12000 *************************\n",
      "loss 15.5639\n",
      "> il est trop vieux pour elle .\n",
      "= he s too old for her .\n",
      "< he s too old for old for old for old\n",
      "\n",
      "************************* iter13000 *************************\n",
      "loss 4.5994\n",
      "> je ne vais pas livrer de noms .\n",
      "= i m not going to name names .\n",
      "< i m not going to lose . <EOS>\n",
      "\n",
      "************************* iter14000 *************************\n",
      "loss 23.1648\n",
      "> c est une fille blonde .\n",
      "= she is a blonde girl .\n",
      "< she s a girl girl girl girl girl girl girl\n",
      "\n",
      "************************* iter15000 *************************\n",
      "loss 9.5005\n",
      "> c est un esprit independant .\n",
      "= she s an independent thinker .\n",
      "< he s a little . <EOS>\n",
      "\n",
      "************************* iter16000 *************************\n",
      "loss 13.2209\n",
      "> il est egoiste et cupide .\n",
      "= he is selfish and greedy .\n",
      "< he s a and . <EOS>\n",
      "\n",
      "************************* iter17000 *************************\n",
      "loss 10.5540\n",
      "> nous ne sommes pas interessees .\n",
      "= we re not interested .\n",
      "< we re not related . <EOS>\n",
      "\n",
      "************************* iter18000 *************************\n",
      "loss 9.1913\n",
      "> je viens du bresil .\n",
      "= i m from brazil .\n",
      "< i m from brazil . . . . . .\n",
      "\n",
      "************************* iter19000 *************************\n",
      "loss 6.9234\n",
      "> je suis endurante .\n",
      "= i m resilient .\n",
      "< i m resilient . <EOS>\n",
      "\n",
      "************************* iter20000 *************************\n",
      "loss 8.6932\n",
      "> nous sommes reconnaissantes .\n",
      "= we re grateful .\n",
      "< we re sorry . . . . . . .\n",
      "\n",
      "************************* iter21000 *************************\n",
      "loss 2.1317\n",
      "> vous etes tres courageuse .\n",
      "= you are very brave .\n",
      "< you re very brave . <EOS>\n",
      "\n",
      "************************* iter22000 *************************\n",
      "loss 5.1181\n",
      "> je suis meilleur que lui .\n",
      "= i m better than him .\n",
      "< i m him him him him him him him him\n",
      "\n",
      "************************* iter23000 *************************\n",
      "loss 27.7130\n",
      "> vous etes fort genereuses .\n",
      "= you re very generous .\n",
      "< you re very generous . <EOS>\n",
      "\n",
      "************************* iter24000 *************************\n",
      "loss 11.7081\n",
      "> elle est en train de l aider .\n",
      "= she is helping him .\n",
      "< she is on the . <EOS>\n",
      "\n",
      "************************* iter25000 *************************\n",
      "loss 7.4580\n",
      "> je suis a ta disposition .\n",
      "= i m at your disposal .\n",
      "< i am at your partner . <EOS>\n",
      "\n",
      "************************* iter26000 *************************\n",
      "loss 0.0484\n",
      "> vous me faites de l ombre .\n",
      "= you re blocking my light .\n",
      "< you re blocking . <EOS>\n",
      "\n",
      "************************* iter27000 *************************\n",
      "loss 33.6970\n",
      "> je suis pret a partir maintenant .\n",
      "= i m ready to leave now .\n",
      "< i m ready to leave . <EOS>\n",
      "\n",
      "************************* iter28000 *************************\n",
      "loss 8.8849\n",
      "> elle est canadienne .\n",
      "= she is canadian .\n",
      "< she is on her . <EOS>\n",
      "\n",
      "************************* iter29000 *************************\n",
      "loss 10.8522\n",
      "> je vais etre ici .\n",
      "= i m going to be over here .\n",
      "< i m going to be here to be here to\n",
      "\n",
      "************************* iter30000 *************************\n",
      "loss 16.9643\n",
      "> j ai trop sommeil pour faire mes devoirs .\n",
      "= i m too sleepy to do my homework .\n",
      "< i m too too too too too too too too\n",
      "\n",
      "************************* iter31000 *************************\n",
      "loss 8.1158\n",
      "> tu es presque aussi grand que tom .\n",
      "= you re almost as tall as tom .\n",
      "< you re almost as <EOS>\n",
      "\n",
      "************************* iter32000 *************************\n",
      "loss 1.6465\n",
      "> elle a ete empoisonnee .\n",
      "= she s been poisoned .\n",
      "< she s been . <EOS>\n",
      "\n",
      "************************* iter33000 *************************\n",
      "loss 7.7320\n",
      "> je ne vais pas te faire de mal .\n",
      "= i m not going to hurt you .\n",
      "< i m not going to hurt you . <EOS>\n",
      "\n",
      "************************* iter34000 *************************\n",
      "loss 1.2682\n",
      "> elles lisent son livre .\n",
      "= they are reading her book .\n",
      "< they are reading her book . <EOS>\n",
      "\n",
      "************************* iter35000 *************************\n",
      "loss 1.1140\n",
      "> vous etes tres sceptique .\n",
      "= you re very skeptical .\n",
      "< you re very skeptical . <EOS>\n",
      "\n",
      "************************* iter36000 *************************\n",
      "loss 8.0608\n",
      "> nous plaisantons .\n",
      "= we re kidding .\n",
      "< we re kidding . <EOS>\n",
      "\n",
      "************************* iter37000 *************************\n",
      "loss 11.1899\n",
      "> je suis presque prete .\n",
      "= i am almost ready .\n",
      "< i am almost ready ready . <EOS>\n",
      "\n",
      "************************* iter38000 *************************\n",
      "loss 9.8212\n",
      "> nous enquetons a ce sujet .\n",
      "= we re investigating it .\n",
      "< we re all we re investigating . <EOS>\n",
      "\n",
      "************************* iter39000 *************************\n",
      "loss 1.6427\n",
      "> tu es endormie .\n",
      "= you re sleepy .\n",
      "< you re sleepy . <EOS>\n",
      "\n",
      "************************* iter40000 *************************\n",
      "loss 2.3780\n",
      "> vous etes fideles .\n",
      "= you re faithful .\n",
      "< you re faithful . <EOS>\n",
      "\n",
      "************************* iter41000 *************************\n",
      "loss 3.5887\n",
      "> il adore la lecture .\n",
      "= he s very fond of reading .\n",
      "< he is very fond of <EOS>\n",
      "\n",
      "************************* iter42000 *************************\n",
      "loss 0.1365\n",
      "> je suis a un pot au boulot .\n",
      "= i m at an office party .\n",
      "< i m at the at the at a at a\n",
      "\n",
      "************************* iter43000 *************************\n",
      "loss 5.4930\n",
      "> il est physicien .\n",
      "= he is a physicist .\n",
      "< he s the talented . <EOS>\n",
      "\n",
      "************************* iter44000 *************************\n",
      "loss 19.0298\n",
      "> vous n etes pas si vieux tom .\n",
      "= you re not that old tom .\n",
      "< you re not that old . <EOS>\n",
      "\n",
      "************************* iter45000 *************************\n",
      "loss 0.0706\n",
      "> je le fais pour ma famille .\n",
      "= i m doing this for my family .\n",
      "< i m doing the doing the doing my my my\n",
      "\n",
      "************************* iter46000 *************************\n",
      "loss 3.1010\n",
      "> je ne vais pas vous le repeter .\n",
      "= i m not going to tell you again .\n",
      "< i m not going to tell you again . <EOS>\n",
      "\n",
      "************************* iter47000 *************************\n",
      "loss 4.6088\n",
      "> c est une pointure dans son domaine .\n",
      "= he s a leading authority in his field .\n",
      "< he s a authority on his authority . <EOS>\n",
      "\n",
      "************************* iter48000 *************************\n",
      "loss 0.3829\n",
      "> il est tres probable qu il vienne .\n",
      "= he s very likely to come .\n",
      "< he is very likely to he is very likely to\n",
      "\n",
      "************************* iter49000 *************************\n",
      "loss 10.0795\n",
      "> nous sommes dos au mur .\n",
      "= we re up against the wall .\n",
      "< we are a . <EOS>\n",
      "\n",
      "************************* iter50000 *************************\n",
      "loss 0.0321\n",
      "> vous n etes pas le bienvenu ici .\n",
      "= you re not welcome here .\n",
      "< you are not welcome here . <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_iters = 50000\n",
    "# n_iters = 1000\n",
    "print_every = 1000\n",
    "plot_every =100\n",
    "\n",
    "plot_losses = []\n",
    "print_loss_total = 0  # Reset every print_every\n",
    "plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]\n",
    "\n",
    "for iter in range(1, n_iters+1):\n",
    "    # Load data\n",
    "    training_pair = training_pairs[iter-1]\n",
    "    input_tensor = training_pair[0]\n",
    "    target_tensor = training_pair[1]\n",
    "    \n",
    "    # Clear gradients w.r.t. parameters\n",
    "    encoder_hidden_forward = encoder_bilinear_forward.initHidden()\n",
    "    encoder_hidden_backward = encoder_bilinear_backward.initHidden()\n",
    "    encoder_optimizer_for.zero_grad()\n",
    "    encoder_optimizer_back.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    encoder_outputs_forward = torch.zeros(MAX_LENGTH, encoder.hidden_dim, device=device) # 10 = MAX_LENGTH\n",
    "    encoder_outputs_backward = torch.zeros(MAX_LENGTH, encoder.hidden_dim, device=device) # 10 = MAX_LENGTH\n",
    "    loss = 0\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    # Forward pass\n",
    "    # encoder\n",
    "    for ei in range(input_length):\n",
    "        encoder_output_forward, encoder_hidden_forward = encoder_bilinear_forward(input_tensor[ei], encoder_hidden_forward)\n",
    "        encoder_outputs_forward[ei] = encoder_output_forward[0,0]  \n",
    "        encoder_output_backward, encoder_hidden_backward = encoder_bilinear_backward(input_tensor[input_length -1 - ei], encoder_hidden_backward)\n",
    "        encoder_outputs_backward[ei] = encoder_output_backward[0,0]\n",
    "    \n",
    "    # decoder    \n",
    "    decoder_hidden = encoder_hidden_forward  + encoder_hidden_backward # 1,1,256\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    is_teacher_forcing = True if random.random() < 0.5 else False\n",
    "    if is_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder_bilinear(decoder_input, decoder_hidden, encoder_outputs_forward)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]\n",
    "    # 여기서는 decoder의 output으로 학습을 이어가기 때문에 decoder의 output이 잘 맞아야 encoder이 잘 학습된다.\n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder_bilinear(decoder_input, decoder_hidden, encoder_outputs_forward)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            # decoder의 출력 값을 다음 입력을 선택 \n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach() \n",
    "            \n",
    "            if decoder_input.item() == EOS_token: # <EOS>가 나오면 반복문을 끝낸다.\n",
    "                break\n",
    "            \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Updating parameters\n",
    "    encoder_optimizer_for.step()\n",
    "    encoder_optimizer_back.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    print_loss_total += loss.item() / target_length\n",
    "    plot_loss_total += loss.item() / target_length\n",
    "\n",
    "    if iter % print_every == 0:\n",
    "        print('*'*25, 'iter%d'%iter, '*'*25)\n",
    "        print('loss %.4f'%loss)\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        evaluateRandomly_bi()\n",
    "\n",
    "    if iter % plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5df25e1460>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#################################################\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과는 좋은데 bidirectional을 완전하게 구현한게 아닌 것 같아서 정확도는 위의 한방향일 때와 별 차이 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EEE4423_lab11_Seq2Seq.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch1.9.0-py3.8-cuda11.1",
   "language": "python",
   "name": "torch1.9.0-py3.8-cuda11.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
